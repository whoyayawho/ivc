다음은 제공된 PDF 파일의 내용을 마크다운 형식으로 변환한 것입니다.

---

## II. 기술개발 목표 및 주요내용

### 1\. 최종 목표 및 주요내용

- **최종목표**
  [cite\_start]4족 로봇 탑재형 고성능 멀티모달 센서 및 공간지능 통합 시스템 개발과 이를 활용한 무인 경계/순찰 실증 [cite: 8]
- **기술개발 목표**
  - [cite\_start]□ 최종 목표: 4족 로봇 탑재형 고성능 멀티모달 센서 및 공간지능 통합 시스템 개발 [cite: 10]
  - □ 정량적 목표
    - [cite\_start]멀티모달 융합 데이터 획득 속도: 20Hz [cite: 12]
    - [cite\_start]멀티모달 융합 데이터 동기화 오차: 10ms 이내 [cite: 13]
    - [cite\_start]3D 렌더링 품질(PSNR): 27 dB 이상 [cite: 14]
    - [cite\_start]이상상황 탐지·보고 성공률: 95% 이상 [cite: 15]
    - [cite\_start]탐지-보고 평균 소요시간: 10초 이내 [cite: 16]
- **실증 내용**
  - [cite\_start]□ 중요시설 무인 순찰 및 경계 [cite: 17]
    - (실증 목표) [cite\_start]지정 구역 자율 순찰, 이상 상황(무단침입, 화재/연기, 시설물 온도 이상 등) 신속 탐지 및 관제 시스템 실시간 보고를 통해 24시간 무인 경계 임무 수행 능력 검증 [cite: 18]
    - (실증 내용)
      - [cite\_start]① 자율 순찰 및 장애물 회피: 지정된 실내외 순찰 경로를 따라 자율 주행하며, 고정/이동 장애물을 실시간으로 회피하는 임무 수행 [cite: 20]
      - [cite\_start]② 침입 탐지 및 추적: 야간 등 저조도 환경에서 미인가 인원을 식별하고, 이동 경로를 관제 시스템에 실시간 전송 [cite: 21]
      - [cite\_start]③ 환경 이상 감지: 열화상 센서를 활용하여 서버실, 전력 설비 등의 온도 이상 및 화재/연기 등 위험 요소를 조기 탐지 [cite: 22]
      - [cite\_start]④ 직관적 관제 및 보고: 사용자가 손쉽게 순찰 경로 설정, 로봇 탐지이상 상황 3D맵 상에 직관적 표시, 자동 경고 보고서 생성 기능 [cite: 23, 24]
- **개발내용**
  - [cite\_start]▶핵심 기술 통합 플랫폼 구축 [cite: 25]
    - [cite\_start]**1차년도** [cite: 26]
      - [cite\_start]다관절 짐벌 기반 센서모듈 통합 및 HW/SW 플랫폼 구축 [cite: 27]
      - [cite\_start]공간지능(Spatial Intelligence) 기반 에이전트 시스템 설계 및 기초 순찰 AI 기술 개발 [cite: 28, 29]
      - [cite\_start]실시간 3D 맵 가시화 인터페이스 프로토타입 개발 [cite: 30]
  - [cite\_start]▶플랫폼 고도화 및 현장 실증 [cite: 31]
    - [cite\_start]**2차년도** [cite: 32]
      - [cite\_start]실시간 멀티모달 센서 융합 및 공간지능 모델 성능 최적화 [cite: 33]
      - [cite\_start]3D Gaussian Splatting 기반 고정밀 맵핑 및 관제 인터페이스 고도화 [cite: 34]
      - [cite\_start]플랫폼 안정화 및 현장 실증 [cite: 35]

---

### 2\. 정량적 기술개발 목표

#### 가. 기술개발 정량적 목표

| 성능지표                               | 단위 | 현재기술수준 비교 | 1차년도  | 기술개발 최종목표<br>2차년도 | 필수           |
| :------------------------------------- | :--- | :---------------- | :------- | :--------------------------- | :------------- | --- |
|                                        |      | **해외**          | **국내** |                              |                |     |
| 1. 멀티모달 융합 데이터 획득 속도(FPS) | Hz   | 10                |          | ≥10                          | ≥20            | O   |
| 2. 멀티모달 융합 데이터 동기화 오차    | ms   | 10                |          | \<20                         | ≤10            | O   |
| 3. 3D 렌더링 품질 (PSNR)               | dB   | 17                |          |                              | ≥27            | O   |
| 4. 이상 상황 탐지 보고 성공률          | %    | 92                |          | ≥95 (실험실 환경)            | ≥95 (실증환경) | Ο   |
| 5. 이상 상황 탐지 보고 평균 소요 시간  | 초   |                   |          | ≤10 (실험실 환경)            | ≤10 (실증환경) | Ο   |

[cite_start][cite: 39]

[cite\_start]○ **현재 기술수준 제시 근거** [cite: 40]

- [cite\_start]**해외** [cite: 41]
  - [cite\_start]• 센서 하드웨어: 개별 센서가 아닌, 여러 센서(RGB, 열화상, LiDAR 등)의 데이터를 실시간으로 융합하여 출력하는 통합 모듈의 성능을 기준으로 함. [cite: 42]
  - [cite\_start]• AI 및 소프트웨어: 국제적으로 공인된 벤치마크 데이터셋에서 보고된 최신 연구(SOTA, State-of-the-Art) 모델들의 성능을 기준으로 함. [cite: 43]
  - [cite\_start]저조도 객체 탐지는 ExDark 데이터셋을 활용한 연구에서 YOLO 계열 모델들이 mAP@0.5 기준 92% 이상의 높은 탐지 성능을 기록. [cite: 44]
- [cite\_start]**국내** [cite: 45]
  - [cite\_start]• 센서 하드웨어 및 AI: 국내 기업들이 개발한 개별 센서 모듈(하이보 iTFS) 및 AI 솔루션의 일반적인 성능 수준을 기준으로 함. [cite: 46]
  - [cite\_start]멀티모달 센서를 하나의 모듈로 통합하거나 AI 모델을 개발하여 상용화한 사례는 아직 제한적임. [cite: 47]
  - [cite\_start]따라서 본 과제는 해외 선도 기술 수준을 목표로 요소 기술들을 효과적으로 통합하고 최적화함으로써, 국내 기술 수준을 한 단계 끌어올리는 것을 목표로 제시. [cite: 48]

---

#### 나. 성능 지표 정의 및 측정 방법

| 성능 지표                               | 시험 정의                                                                                                                                                    | 측정 방법                                                                                                                                                                                                                                                                                                                                           |
| :-------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1. 멀티모달 융합 데이터 획득 속도 (FPS) | 융합된 멀티모달 데이터가 동기화되어 초당 출력되는 프레임수.                                                                                                  | [cite\_start]데이터 수집 소프트웨어를 통해 60초간 수신되는 동기화된 데이터 프레임 수를 60초로 나누어 측정 [cite: 51]                                                                                                                                                                                                                                |
| 2. 멀티모달 융합 데이터 동기화 오차     | 서로 다른 센서로부터 수집된 데이터가 시간적으로 얼마나 정확히 일치하는지를 나타내는 지표.                                                                    | 모든 센서가 감지할 수 있는 외부 이벤트(예: LED 점멸)를 발생시켜 동시 측정. [cite\_start]각 센서 데이터에 기록된 이벤트 시점의 타임스탬프 간 절대 시간 차이를 반복 측정하여 평균 오차를 계산. [cite: 51]                                                                                                                                             |
| 3. 3D 렌더링 품질 (PSNR)                | 생성된 3D 모델에서 렌더링한 가상 시점 이미지와 실제 촬영된 이미지(Ground Truth) 간의 유사도를 측정하는 지표. 값이 높을수록 원본에 가까운 고품질 복원을 의미. | 로봇이 맵핑용 데이터를 수집하는 과정에서 촬영한 이미지 중 일부를 테스트용(Ground Truth)으로 선별. 해당 이미지가 촬영된 시점을 제외하고 3D 맵을 생성한 후, 선별된 이미지와 동일한 시점에서 이미지를 렌더링. [cite\_start]렌더링된 이미지와 원본 이미지를 픽셀 단위로 비교하여 PSNR 값을 계산하고, 모든 테스트 시점의 평균 PSNR 값을 기록. [cite: 51] |
| 4. 이상 상황 탐지 보고 성공률           | 의도적으로 발생시킨 이상 상황(침입, 온도 이상 등)에 대해 시스템이 이를 정확히 탐지하고 관제 시스템에 성공적으로 보고한 비율.                                 | [cite\_start]실험실 환경(1차년도) 또는 최종 실증 환경(2차년도)에서 20건 이상의 이상 상황 시나리오 재현, 각 시나리오에 대해 로봇이 탐지하고 관제 시스템에 경고 및 관련 데이터의 표시·기록 여부를 확인하여 성공률(%)로 계산. [cite: 51]                                                                                                               |
| 5. 이상 상황 탐지 보고 평균 시간        | 로봇이 현장에서 이상 상황을 인지한 시점부터 관제 시스템 화면에 경고가 표시되기까지 소요되는 평균 시간.                                                       | [cite\_start]실험실 환경(1차년도) 또는 최종 실증 환경(2차년도)에서 20건 이상의 이상 상황 시나리오 재현, 로봇의 센서 데이터에 이벤트가 기록된 타임스탬프와 관제 시스템에 경고가 표시된 타임스탬프 간의 차이를 측정하여 평균값 계산. [cite: 51]                                                                                                       |

---

### 3\. 단계별 세부 추진내용

(그림 11. '인지 판단 모듈 플랫폼' 개발 내용: '다관절 짐벌 기반 센서 모듈 통합 플랫폼 구축'에서 시작하여 '공간지능 모델 성능 최적화' 및 '고정밀 맵핑 및 관제 인터페이스 고도화'를 거쳐 '실증 평가'로 이어지는 다이어그램. 각 단계는 '다관절 짐벌 기반 센서 모듈', '공간지능 기반 객체 인지/추적', '장시간 연속 무인순찰 검증' 등의 핵심 구성요소를 포함함.) [cite_start][cite: 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65]

#### 가. [cite\_start]1차년도 추진내용: 핵심 기술 통합 플랫폼 구축 [cite: 66]

[cite\_start]○ **다관절 짐벌 기반 센서모듈 통합 플랫폼 구축** [cite: 67]

- (하드웨어) [cite\_start]로봇의 자세 변화에도 센서의 시야를 안정적으로 유지하기 위한 다관절 짐벌 메커니즘 설계/제작 및 멀티모달센서 모듈(LiDAR, RGB 카메라, 열화상 카메라, IMU) 탑재 [cite: 68]
- (캘리브레이션) [cite\_start]카메라의 왜곡 보정을 위한 내부 파라미터(Intrinsic)와 각 센서(LiDAR, 카메라, IMU) 간의 좌표계를 정합하는 외부 파라미터(Extrinsic) 캘리브레이션을 수행하여 데이터 융합의 정확도 확보 [cite: 69, 70]
- (소프트웨어) [cite\_start]통합 소프트웨어 프레임워크를 설계하고, 짐벌 제어 시스템, 센서 데이터 취득 및 동기화, 멀티모달 융합 데이터 생성(RGB-열화상-3D 포인트 클라우드 데이터), 4족 로봇 연동 및 제어를 위한 기본 모듈 개발 [cite: 71]

[cite\_start](그림 12. Ambient Image를 이용한 캘리브레이션: '기존' [cite: 73] [cite\_start]방식과 '개선' [cite: 74]된 방식의 캘리브레이션 결과를 비교하는 이미지)

[cite\_start]○ **공간지능 기반 에이전트 시스템 설계 및 기초 순찰 AI 기술 개발** [cite: 76]

- (에이전트 시스템 설계) [cite\_start]로봇이 주변 환경을 인식하고, 스스로 판단하여 임무를 수행하는 공간지능 기반 에이전트 시스템 아키텍처 설계 [cite: 77]
- (SLAM) [cite\_start]LiDAR, IMU 센서 데이터를 융합하여 실내외 환경의 3D 점군(Point Cloud) 지도를 생성하는 SLAM 기술 개발 [cite: 78, 79]
- (자율주행) [cite\_start]생성된 3D 지도를 기반으로 경로를 계획하고, 실시간 센서 정보로 장애물을 회피하며 목표 지점까지 이동하는 자율주행 알고리즘 개발 [cite: 80]
- (객체탐지) [cite\_start]멀티모달 센서를 통해 사람을 포함한 기본 객체를 탐지하는 AI 모델을 개발하고 플랫폼에 탑재 [cite: 81]

[cite\_start](그림 13. 공간지능 기반 에이전트 시스템 구조 개념도: 센서 [cite: 82] [cite\_start]입력부터 인지 모듈 [cite: 87][cite\_start], Agent(추론/계획) [cite: 90][cite\_start], 기억 [cite: 92][cite\_start], Tools [cite: 91] [cite\_start]등을 거쳐 '작동 명령' [cite: 94] [cite\_start]또는 '대화 임무판단/상태 보고' [cite: 84, 85]로 이어지는 AI 플랫폼 내부 구조도)

[cite\_start]○ **실시간 3D 맵 가시화 인터페이스 프로토타입 개발** [cite: 98]

- (3D 가시화) [cite\_start]SLAM으로 생성된 3D 지도와 로봇의 현재 위치, 센서 데이터를 실시간으로 시각화하는 뷰어(Viewer) 개발 [cite: 99]
- (원격 제어 및 모니터링) [cite\_start]사용자가 3D 지도 상에 순찰 경로를 설정하고, 로봇의 상태(배터리, 통신 등)를 시각화하고, 원격으로 로봇을 제어할 수 있는 3D 관제 인터페이스 프로토타입 개발 [cite: 100]

---

#### 나. [cite\_start]2차년도 추진내용: 플랫폼 고도화 및 현장 실증 [cite: 101]

[cite\_start]○ **실시간 멀티모달 센서 융합 및 공간지능 모델 성능 최적화** [cite: 102]

- (알고리즘 고도화) [cite\_start]LiDAR, 카메라, IMU 등 이종 센서 데이터를 실시간으로 정합·보정하여 센서 간 시간·공간적 오차를 최소화하고 환경 인식의 일관성과 안정성을 확보하는 데이터 융합 알고리즘 고도화 [cite: 103]
- (탐지 성능 향상) [cite\_start]실환경에서 확보된 데이터 및 유사 환경 데이터를 활용하여, 야간 환경에서의 침입자, 특정 시설물의 온도 이상, 화재/연기 등 위험요소에 대한 탐지 AI 모델의 정확도와 신뢰성 향상 [cite: 104]
- (추적 기능 개발) [cite\_start]탐지된 특정 객체(미인가자 등)를 로봇이 일정 거리 유지하며 따라가는 추적(Tracking) 기능 개발 [cite: 105]

(그림 14. 특정 객체(사람) [cite\_start]인지 및 추적 예시: 3D 포인트 클라우드 환경 내에서 사람 객체를 인지하고 추적하는 모습을 시각화한 이미지) [cite: 106]

[cite\_start]○ **3D Gaussian Splatting 기반 고정밀 맵핑 및 관제 인터페이스 고도화** [cite: 107]

- (고정밀 맵핑) [cite\_start]3D Gaussian Splatting 기술을 적용하여, 순찰 공간을 사실적이고 정밀한 3D 모델로 재구성하고, 이를 통해 공간지능의 상황인지 능력 향상 [cite: 109]
- (상황인지 강화) [cite\_start]멀티모달센서가 탐지한 이상 상황(침입자, 위험요소 등)을 3D맵 상에 직관적인 아이콘과 함께 표시하고, 관련 정보를 실시간으로 사용자에게 전달하는 기능을 개발 [cite: 109]
- (자연어 이해) [cite\_start]사용자의 음성 또는 텍스트 명령('순찰 시작해', '배터리 상태 알려줘' 등)을 이해하고 의도를 파악하는 LLM 기반 자연어 모듈 개발 [cite: 110]
- (보고서 자동화) [cite\_start]탐지된 이벤트의 시간, 위치, 영상 등의 데이터를 기반으로 순찰 결과 보고서를 자동으로 생성하고 이력을 관리하는 기능 개발 [cite: 111]

[cite\_start](그림 15. 가우시안 스플래팅 기법 적용 예시: 기존의 '3D 포인트 클라우드' [cite: 112] [cite\_start]방식과 '3D Gaussian Splatting' [cite: 113] 기술로 구현한 3D 환경을 비교하는 이미지)

[cite\_start]○ **플랫폼 안정화 및 현장 실증** [cite: 115]

- (시스템 최적화) [cite\_start]장시간(4시간 이상) 안정적인 무인 순찰 임무 수행을 위해 시스템 전력 관리 효율을 개선하고, 소프트웨어 안정성 확보 [cite: 116]
- (현장 실증) [cite\_start]실제 중요시설 또는 유사한 환경에서 개발된 플랫폼의 통합 성능 및 운용성을 평가하고, 정량적 목표 달성 여부 최종 검증 [cite: 117]

---

### [cite\_start]4. 실증 로드맵 [cite: 119]

(그림 16. 실증 로드맵: 1단계(핵심 기능 통합 검증) [cite_start][cite: 120, 168][cite\_start], 2단계(시나리오 기반 성능 검증) [cite: 121, 126][cite\_start], 3단계(최종 운용 신뢰성 검증) [cite: 128, 129]로 구성된 로드맵. [cite\_start]각 단계별 실증 내용 [cite: 146][cite\_start], 평가 항목 [cite: 147][cite\_start], 실증 장소 [cite: 165]가 명시됨.)

[cite\_start]○ **1단계 (1차년도 4Q): 핵심 기능 통합 검증 (실험실 환경)** [cite: 168]

- [cite\_start]**실증 목표** [cite: 169]
  - [cite\_start]다양한 센서로부터 수집된 데이터의 정합성 및 처리 성능 검증 [cite: 170]
  - [cite\_start]개발된 핵심 기능(SLAM 기반 자율주행, 객체 탐지 등)을 통합하고, 실험실 수준에서 검증 [cite: 171]
  - [cite\_start]기본 관제 인터페이스와의 연동성을 확인하여 시스템 전체의 안정성 확보 [cite: 172]
- [cite\_start]**실증 장소** [cite: 173]
  - [cite\_start]참여기관 내 테스트베드 (실험실 및 통제된 실내 환경) [cite: 174]
- [cite\_start]**실증 내용** [cite: 175]
  - [cite\_start]**센서 데이터 동기화 정확도 검증** [cite: 176]
    - [cite\_start]• 로봇에 탑재된 멀티모달센서가 동일한 물리적 현상을 동일 시점에 포착하는지 검증 [cite: 178]
    - [cite\_start]• 모든 센서가 감지할 수 있는 외부 이벤트(예: LED 점멸)를 발생시켜, 각 센서 데이터에 기록된 이벤트의 타임스탬프를 비교·분석 [cite: 179]
    - [cite\_start]• 센서 간 타임스탬프 오차가 목표 시간 이내로 유지되는지 반복 측정하여, 3D 지도 생성 및 동적 객체 인식의 기반이 되는 데이터 융합의 신뢰도를 확보 [cite: 180]
  - [cite\_start]**통합 데이터 수집 성능 검증** [cite: 181]
    - [cite\_start]• 로봇이 실제 주행하는 상황을 가정하여, 모든 센서를 최대 성능으로 활성화한 상태에서 60초 이상 연속적으로 데이터 수집 [cite: 182]
    - [cite\_start]• 이 과정에서 발생하는 대용량의 포인트 클라우드, 고해상도 이미지, 관성 측정 데이터 등의 멀티모달 통합 데이터가 프레임 손실 없이 안정적으로 저장되고 처리되는지 확인 [cite: 183]
    - [cite\_start]• 초당 획득 프레임 수(FPS)를 측정하여 시스템의 데이터 처리량이 목표 사양을 만족하는지 평가하고, 실시간 자율주행에 필요한 데이터 파이프라인의 안정성을 보장 [cite: 184]
  - [cite\_start]**실시간 3D 환경 지도 작성 및 정합성 평가** [cite: 186]
    - [cite\_start]• 로봇이 미지의 테스트베드 환경을 주행하며, 멀티모달센서 데이터를 실시간으로 융합하여 수집한 3D 포인트 클라우드 지도의 완성도 검증 [cite: 187]
    - [cite\_start]• 생성된 지도가 실제 공간의 벽, 기둥, 집기 등 주요 구조물의 형태와 크기를 얼마나 정확하게 반영하는지 정합성을 평가 [cite: 188]
    - [cite\_start]• 로봇이 처음 접하는 공간에서도 환경을 정확하게 인식하고 3D 포인트 클라우드 지도를 구축할 수 있는 능력을 확인 [cite: 189]
  - [cite\_start]**지도 기반 위치 인식 정확도 측정** [cite: 192]
    - [cite\_start]• 사전에 정밀하게 구축된 3D 지도를 기반으로, 로봇이 주변 환경 정보를 스캔하여 지도상에서 자신의 현재 위치와 방향을 빠르고 정확하게 찾는 성능을 검증 [cite: 193]
    - [cite\_start]• 위치 인식 알고리즘이 추정한 위치/자세를 실제 주행 궤적 간의 평균 오차를 측정하여 정밀도 평가 [cite: 193]
  - [cite\_start]**장애물 인식 및 동적 회피 기동 검증** [cite: 198]
    - [cite\_start]• 돌발 상황에 대한 로봇의 지능적인 대처 능력과 안정성을 검증 [cite: 199]
    - [cite\_start]• 로봇의 자율주행 경로 상에 다양한 형태의 정적 장애물(박스, 의자 등)과 동적 장애물(보행자, 다른 이동체)을 예고 없이 배치하고, 로봇이 이를 실시간으로 정확하게 인식하고, 충돌 위험을 판단하여 부드럽게 감속 및 정지하거나 최적의 우회 경로를 동적으로 재계획하여 끊김없이 임무를 지속하는지 검증 [cite: 200, 202]
  - [cite\_start]**다양한 조도 환경에서의 AI 기반 이상 상황 탐지 성능 검증** [cite: 204]
    - [cite\_start]• 정상적인 주간 조명 환경뿐만 아니라, 빛이 거의 없는 야간 환경을 조성하여 AI 탐지 모델의 강건성을 시험 [cite: 205]
    - [cite\_start]• 설정된 시나리오(예: 특정 구역에 사람이 침입하는 상황)를 주/야간 환경에서 각각 20회 이상 반복 재현하고, 탐지 성공률과 오탐지, 미탐지 사례를 분석하여 모델의 신뢰도 검증 [cite: 206]
  - [cite\_start]**실시간 경고 전송 및 관제 시스템 연동 응답 시간 측정** [cite: 207]
    - [cite\_start]• 로봇이 이상 상황을 탐지한 시점부터 이벤트 정보(종류, 발생 위치, 실시간 영상)가 무선 네트워크를 통해 관제 시스템에 전달되어 운영자 화면에 경고 알림으로 표시되기까지의 전체 소요 시간을 측정 [cite: 208]
    - [cite\_start]• 이 테스트를 통해 탐지-전송-표시로 이어지는 전체 정보 전달 파이프라인의 효율성을 검증하고, 위급 상황 발생 시 운영자가 신속하게 상황을 인지하고 대응할 수 있음을 보장 [cite: 209]
  - [cite\_start]**원격 관제 인터페이스의 양방향 제어 기능 실증** [cite: 210, 211]
    - [cite\_start]• 원격지의 운영자가 3D 관제 인터페이스를 통해 로봇의 현재 상태(실시간 위치, 배터리 잔량, 센서 상태)를 직관적으로 모니터링할 수 있는지 확인 [cite: 212, 213]
    - [cite\_start]• 지정된 경로로 순찰을 시작/중지/재개시키거나, 특정 지점으로 강제 이동시키는 등의 원격 명령을 전송하고 로봇이 해당 명령을 정확하게 수행하는지 테스트 후 시스템의 기본적인 원격 운용성을 검증 [cite: 214, 215]
- [cite\_start]**성능 검증 기준** [cite: 216]
  - [cite\_start]• 멀티모달 융합 데이터 획득 속도: 10Hz 이상 (초당 프레임 수) [cite: 217]
  - [cite\_start]• 멀티모달 융합 데이터 동기화 오차: 20ms 이내 [cite: 218]
  - [cite\_start]• 이상 상황(침입) 탐지 보고 성공률: 95% 이상 [cite: 219]
  - [cite\_start]• 이상 상황(침입) 탐지 보고 평균시간: 10초 이내 [cite: 220]

[cite\_start]○ **2단계 (2차년도 2Q): 시나리오 기반 성능 검증 (유사 환경)** [cite: 221]

- [cite\_start]**실증 목표** [cite: 222]
  - [cite\_start]1단계에서 확보한 기술과 데이터를 기반으로 가우시안 스플래팅(Gaussian Splatting) 기술을 적용하여 실제와 유사한 3D 환경 구축 [cite: 224]
  - [cite\_start]고품질 3D 환경을 기반으로 실내외 복합 순찰 및 돌발 상황 대응 시나리오를 수행하고, 고도화된 AI 기능의 실효성과 장시간 운용에 따른 시스템 안정성 검증 [cite: 225]
- [cite\_start]**실증 장소** [cite: 226]
  - [cite\_start]실제 운용 환경과 유사한 야외 환경 (대학캠퍼스 등) [cite: 227]
- [cite\_start]**실증 내용** [cite: 228]
  - [cite\_start]**사실적 3D 환경 구축 및 품질 평가** [cite: 229]
    - [cite\_start]• 로봇이 순찰하며 수집한 고해상도 RGB 이미지 데이터와 SLAM으로 취득한 정확한 위치 데이터를 결합하여, 포인트 클라우드를 넘어선 실사 수준의 3D 환경 모델 생성 [cite: 230]
    - [cite\_start]• 실제 특정 지점에서 촬영한 원본 이미지와 3D 모델에서 동일한 시점으로 렌더링한 이미지를 픽셀 단위로 비교하여 PSNR(최대 신호 대 잡음비) 값 측정 [cite: 231]
    - [cite\_start]• 3D 환경이 실제 공간 표현 정밀도를 객관적인 지표로 평가하고, 관제 요원의 직관적 상황 인지를 위한 시각화 기술의 성능을 검증 [cite: 231]
  - [cite\_start]**실내외 복합 환경에서의 자율주행 및 장애물 극복 능력 검증** [cite: 237]
    - [cite\_start]• 건물 내부에서 시작하여 경사로를 지나 외부를 주행하고 다시 건물로 복귀하는 등, 실내와 실외가 연결된 복잡한 장거리 순찰 경로를 설정 [cite: 238, 239]
    - [cite\_start]• 로봇이 환경 변화(조도, 바닥 재질 등)에 강건하게 대응하며 끊김 없이 자율주행하는지 테스트 [cite: 240]
    - [cite\_start]• 이 과정에서 예측하기 어려운 다양한 유형의 장애물(예: 갑자기 열리는 문, 통행인)에 대한 극복 능력을 확인함으로써, 1단계보다 역동적이고 변화가 많은 실제 환경에서의 SLAM 강건성과 주행 안정성을 종합적으로 평가 [cite: 241]
  - [cite\_start]**고품질 3D 환경을 활용한 침입자 추적 시나리오 수행** [cite: 243, 244]
    - [cite\_start]• 로봇은 침입자를 즉시 탐지하고, 단순 경고를 넘어 가우시안 스플래팅으로 구축된 사실적인 3D 지도 위에 침입자의 실시간 이동 경로를 시각적으로 표시하며 추적하는 임무를 수행 [cite: 245]
    - [cite\_start]• 관제 요원이 침입자 동선을 직관적으로 파악하고 신속하게 대응할 수 있도록 고품질 3D 시각화 기술의 성능 검증 [cite: 246]
  - [cite\_start]**열화상 센서 연동 시설물 이상 온도 탐지 및 보고** [cite: 247]
    - [cite\_start]• 순찰 경로에 포함된 주요 전력 설비, 서버실 등 온도에 민감한 시설물을 대상으로 열화상 카메라를 이용한 자동 점검 수행 [cite: 248]
    - [cite\_start]• 사전에 설정된 정상 온도 범위를 초과하는 지점이 감지될 경우, 시스템은 이를 즉시 이상 상황으로 판단하여 경고 알림을 생성 [cite: 249]
    - [cite\_start]• 동시에 3D 지도상에 이상이 감지된 설비의 정확한 위치와 현재 온도를 동시에 표시하여 운영자가 원격에서도 문제 지점을 신속하고 명확하게 파악할 수 있는지 검증 [cite: 250]
- [cite\_start]**성능 검증 기준** [cite: 251]
  - [cite\_start]• 멀티모달 융합 데이터 획득 초당 프레임 수: 20Hz 이상 [cite: 252]
  - [cite\_start]• 멀티모달 융합 데이터 동기화 오차: 10ms 이내 [cite: 253]
  - [cite\_start]• 3D 렌더링 품질 (PSNR): 27dB 이상 [cite: 254]

[cite\_start]○ **3단계 (2차년도 4Q): 최종 운용 신뢰성 검증 (실증 현장)** [cite: 255]

- [cite\_start]**실증 목표** [cite: 256, 257]
  - [cite\_start]최적화된 가우시안 스플래팅 기반 3D 관제 시스템을 포함한 최종 플랫폼을 보안이 요구되는 실제 수요처 환경에 투입 [cite: 258]
  - [cite\_start]장시간 연속 무인 임무 수행을 통해 시스템의 통합 성능, 안정성, 현장 운용 편의성을 종합적으로 검증하여 최종 상용화 단계인 기술성숙도(TRL) 8단계 달성 [cite: 259]
- [cite\_start]**실증 장소** [cite: 260]
  - [cite\_start]실증 목표 지역 (보안이 요구되는 연구시설, 대규모 전산실 등 중요시설) [cite: 261]
- [cite\_start]**실증 내용** [cite: 262]
  - [cite\_start]**장시간 연속 무인 순찰을 통한 시스템 신뢰성 검증** [cite: 263]
    - [cite\_start]• 실제 운용 환경에서 4시간 이상 충전 없이 연속으로 무인 순찰 임무를 수행하여 장시간 구동 시 발생할 수 있는 소프트웨어적 문제(인지, 메모리 누수, 시스템 오류 등)와 하드웨어적 문제(발열, 배터리 성능 저하, 구동부 내구성) 집중 점검 [cite: 265]
    - [cite\_start]• 24시간 365일 무중단 운영을 위한 플랫폼의 최종적인 안정성과 신뢰도 검증 [cite: 265]
  - [cite\_start]**최종 AI 모델의 이상 상황 탐지 및 보고 성능 검증** [cite: 266]
    - [cite\_start]• 실제 운용 환경에서 발생 가능한 이상 상황 시나리오(미등록 인원 침입, 설비 과열, 비정상적 소음 발생 등)를 20회 이상 재현 [cite: 267]
    - [cite\_start]• 1, 2단계를 거치며 고도화된 최종 AI 모델의 탐지 보고 성능을 측정하고, 시스템의 탐지 정확성과 보고 신속성이 상용화 수준에 도달했음을 증명 [cite: 268]
  - [cite\_start]**3D 환경 모델의 최종 품질 측정** [cite: 269]
    - [cite\_start]• 실제 수요처 환경에서 장시간 운용하며 누적된 대량의 데이터를 활용하여 가우시안 스플래팅 3D 환경 모델을 최종적으로 최적화 [cite: 270]
    - [cite\_start]• 시각화 품질의 성능 측정을 통해 시스템의 확장성과 우수성을 객관적으로 입증 [cite: 271]
  - [cite\_start]**현장 운용자 중심의 사용 편의성 및 내환경성 실증** [cite: 272]
    - [cite\_start]• 최종 개발된 가우시안 스플래팅 기반 3D 관제 인터페이스를 실제 현장 운용자가 직접 사용하여 순찰 경로를 생성·편집하고, 이상 상황 보고를 접수해 후속 조치를 취하는 등 전체 운용 프로세스를 검증 [cite: 273]
    - [cite\_start]• 전문가가 아닌 일반 운용자 관점에서의 직관성과 운용 편의성을 평가하여, 시스템의 실효성과 신뢰도를 종합적으로 검증 [cite: 274]

(그림 21. 관제 인터페이스 예시: 로봇의 상태, 실시간 비디오 피드, 3D 지도 및 경로 등이 표시된 관제 시스템 대시보드. 자료출처: 인티그리트) [cite_start][cite: 275, 276, 277, 278, 279, 280, 281, 283]

- [cite\_start]**성능 검증 기준:** [cite: 284]
  - [cite\_start]이상 상황 탐지 보고 성공률: 95% 이상 [cite: 285]
  - [cite\_start]이상 상황 탐지 보고 평균 시간: 10초 이내 [cite: 285]
