# IVC (혁신비전센터) 신입 직원 소개 발표 자료

**발표 시간**: 30분 (발표 25분 + Q&A 5분)
**대상**: 에스오에스랩 신입 직원
**발표자**: IVC 부서

---

## 📋 발표 구성

1. Opening (1분)
2. Physical AI 시대의 도래 (5-6분)
3. IVC의 탄생과 비전 (3-4분)
4. 공간지능 플랫폼: SPADI (12-13분)
5. 기술 로드맵과 미래 (3-4분)
6. Closing (1분)

**총 예상 슬라이드**: 16-18장

---

# 발표 내용

## 1. Opening (1분, 1장)

### 슬라이드 1: 타이틀

```
IVC (혁신비전센터) 소개

Innovation Vision Center

"센서가 공간을 스캔하는 것을 넘어,
 공간을 이해하게 만들자"

Spatial AI로 Physical AI를 완성한다
```

**발표 멘트**:

> 안녕하십니까. IVC 혁신비전센터입니다. 오늘은 신입 여러분들께 IVC가 무엇을 하는 곳인지, 왜 만들어졌는지, 그리고 어떤 미래를 만들어가고 있는지 말씀드리겠습니다.
>
> 우리의 핵심 비전은 "센서가 공간을 스캔하는 것을 넘어, 공간을 이해하게 만들자"입니다. 즉, Spatial AI, 공간지능으로 Physical AI를 완성하는 것이 우리의 목표입니다.

---

## 2. Physical AI 시대의 도래 (5-6분, 3장)

### 슬라이드 2: Physical AI란 무엇인가?

```
Physical AI 시대의 도래

Physical AI란?
- 물리적 세계에서 인간처럼 움직이고 작업하는 AI
- 산업용 로봇, 서비스 로봇, 자율주행 등

시장 전망
- 글로벌 로봇 시장: 2030년까지 $200B+ 예상
- AI 로봇 시장 연평균 성장률 25%+
- Physical AI는 차세대 AI의 핵심 영역

주요 플레이어
- Boston Dynamics (4족/2족 로봇)
- Tesla (휴머노이드 Optimus, 자율주행)
- NVIDIA (Isaac 플랫폼, Omniverse)
- Figure AI, 1X Technologies (휴머노이드)
```

**발표 멘트**:

> 요즘 AI 업계에서 가장 뜨거운 키워드 중 하나가 바로 Physical AI입니다.
>
> Physical AI란 물리적 세계에서 인간처럼 움직이고 작업을 수행하는 AI를 말합니다. Boston Dynamics의 4족 로봇, Tesla의 휴머노이드 Optimus, 그리고 다양한 자율주행 시스템들이 대표적인 예입니다.
>
> 시장도 폭발적으로 성장하고 있습니다. 글로벌 로봇 시장은 2030년까지 2,000억 달러 이상으로 성장할 것으로 예상되며, AI 로봇 시장은 연평균 25% 이상 성장하고 있습니다.

---

### 슬라이드 3: Physical AI의 한계와 도전

```
Physical AI가 직면한 근본적인 문제

문제 1: "보는 것" ≠ "이해하는 것"
├─ 단순 센서 데이터 수집 (포인트 클라우드, 이미지)
└─ 공간의 의미, 맥락, 관계를 이해하지 못함

문제 2: 환경 변화에 취약
├─ 조도 변화 (밤/낮, 실내/실외)
├─ 날씨 변화 (비, 안개, 눈)
└─ 동적 환경 (사람, 장애물 이동)

문제 3: 자율적 판단 부족
├─ "여기로 가라" (명령 수행) ✓
└─ "침입자를 찾아서 추적하라" (상황 판단 + 자율 행동) ✗

→ Physical AI가 제대로 작동하려면
   공간을 "종합적으로 이해"하는 능력이 필요
```

**발표 멘트**:

> 그런데 Physical AI는 아직 근본적인 한계가 있습니다.
>
> 첫째, "보는 것"과 "이해하는 것"은 다릅니다. 현재 대부분의 로봇은 센서로 데이터를 수집할 수는 있지만, 그 공간이 무엇을 의미하는지, 어떤 맥락인지 이해하지 못합니다.
>
> 둘째, 환경 변화에 매우 취약합니다. 조도가 바뀌거나, 날씨가 바뀌거나, 사람들이 움직이는 동적 환경에서는 제대로 작동하지 못하는 경우가 많습니다.
>
> 셋째, 자율적 판단 능력이 부족합니다. "여기로 가라"는 명령은 수행할 수 있지만, "침입자를 찾아서 추적하라"처럼 상황을 판단하고 자율적으로 행동해야 하는 복잡한 작업은 어렵습니다.
>
> 결국 Physical AI가 제대로 작동하려면, 공간을 종합적으로 이해하는 능력이 필요합니다.

---

### 슬라이드 4: Spatial AI의 필요성

```
Physical AI를 완성하는 핵심: Spatial AI

Physical AI vs Spatial AI

Physical AI                  Spatial AI
"움직이는 것"               "보고, 생각하고, 판단하는 것"
하드웨어 제어                공간 인지 및 이해
명령 실행                    자율적 의사결정


왜 Spatial AI가 필요한가?

1. 공간 재구성 (Reconstruction)
   └─ 실제 환경을 고정밀 3D로 재구성
   └─ 시뮬레이션 환경 생성 (Real2Sim)

2. 공간 이해 (Understanding)
   └─ 객체 감지, 위치 추적, 의미적 맥락 파악
   └─ "무엇이, 어디에, 어떤 상태로 있는가"

3. 공간 추론 (Reasoning)
   └─ 상황 판단 및 자율적 의사결정
   └─ "다음에 무엇을 해야 하는가"


→ Spatial AI는 Physical AI의 "눈"이자 "두뇌"
```

**발표 멘트**:

> 바로 이 지점에서 Spatial AI, 즉 공간지능이 필요합니다.
>
> Physical AI가 "사람처럼 움직이는 것"에 집중한다면, Spatial AI는 "사람처럼 보고, 생각하고, 판단하는 것"에 집중합니다.
>
> Spatial AI는 크게 세 가지 역할을 합니다.
>
> 첫째, 공간 재구성. 실제 환경을 고정밀 3D로 재구성하고, 시뮬레이션 환경을 만듭니다.
>
> 둘째, 공간 이해. 객체를 감지하고 위치를 추적하며, 의미적 맥락을 파악합니다. "무엇이, 어디에, 어떤 상태로 있는가"를 아는 것입니다.
>
> 셋째, 공간 추론. 상황을 판단하고 자율적으로 의사결정합니다. "다음에 무엇을 해야 하는가"를 스스로 결정하는 것입니다.
>
> 즉, Spatial AI는 Physical AI의 "눈"이자 "두뇌"입니다.

---

## 3. IVC의 탄생과 비전 (3-4분, 2장)

### 슬라이드 5: 에스오에스랩의 강점과 IVC 설립

```
에스오에스랩의 도전: 센서를 넘어 플랫폼으로

에스오에스랩의 강점
├─ 솔리드 스테이트 라이다 설계/제조
├─ 3D 비전 센서 분야 하드웨어 전문성
└─ 오랜 기간 축적된 라이다 기술력


새로운 도전
단순 센서 제조 → 혁신적인 솔루션 개발
Hardware → Hardware + Software 통합


IVC 설립 (2025년)
목적: 에스오에스랩의 미래 먹거리 발굴
     차세대 기술 트렌드 선도

핵심 통찰
"단순히 공간을 스캔하는 센서 기술에서 벗어나,
 공간을 이해하는 지능형 플랫폼을 개발하자"
```

**발표 멘트**:

> IVC는 바로 이러한 시장 상황과 기술적 필요성을 배경으로 탄생했습니다.
>
> 에스오에스랩은 솔리드 스테이트 라이다를 설계하고 제조하는 기업으로, 3D 비전 센서 분야에서 강력한 하드웨어 전문성을 보유하고 있습니다.
>
> 하지만 우리는 여기서 한 걸음 더 나아가고자 했습니다. 단순히 센서를 만드는 것을 넘어, 하드웨어와 소프트웨어를 통합한 혁신적인 솔루션을 개발하자는 것이죠.
>
> 그래서 2025년 IVC가 설립되었습니다. 에스오에스랩의 미래 먹거리를 발굴하고, 차세대 기술 트렌드를 선도하기 위해서입니다.
>
> 우리의 핵심 통찰은 이것입니다. "단순히 공간을 스캔하는 센서 기술에서 벗어나, 공간을 이해하는 지능형 플랫폼을 개발하자"

---

### 슬라이드 6: IVC의 비전과 차별화 전략

```
IVC의 비전과 전략

핵심 비전
"Spatial AI로 Physical AI를 완성한다"

├─ 멀티모달 센서 기반 공간지능 플랫폼 개발
├─ Physical AI 시대를 대비한 기술 리더십 확보
└─ 공간 정보를 Physical AI가 이해할 수 있는 형태로 변환


차별화 전략

1. 멀티모달 센서 융합
   └─ LiDAR + RGB/열화상 카메라 + IMU 통합
   └─ 단일 센서 한계 극복

2. AI 기반 공간지능 플랫폼
   └─ LLM + 컴퓨터 비전 + 딥러닝
   └─ 자연어로 명령하고 자율적으로 판단

3. Hardware + Software 완전 통합
   └─ 센서부터 AI까지 End-to-End 솔루션
   └─ Physical AI 시스템에 Plug-and-Play


핵심 역량
- 멀티모달 센서 융합 기술
- 공간지능(Spatial AI) 플랫폼
- LLM 기반 자율 에이전트 시스템
- 3D 재구성 및 시각화
```

**발표 멘트**:

> IVC의 핵심 비전은 "Spatial AI로 Physical AI를 완성한다"입니다.
>
> 우리의 차별화 전략은 세 가지입니다.
>
> 첫째, 멀티모달 센서 융합입니다. 라이다만, 카메라만 사용하는 것이 아니라, LiDAR, RGB 카메라, 열화상 카메라, IMU를 모두 통합하여 단일 센서의 한계를 극복합니다.
>
> 둘째, AI 기반 공간지능 플랫폼입니다. LLM, 컴퓨터 비전, 딥러닝을 결합하여 자연어로 명령하면 AI가 스스로 판단하고 행동합니다.
>
> 셋째, 하드웨어와 소프트웨어를 완전히 통합한 End-to-End 솔루션을 제공합니다. 다양한 Physical AI 시스템에 Plug-and-Play 방식으로 장착할 수 있습니다.

---

## 4. 공간지능 플랫폼: SPADI (12-13분, 6-7장)

### 슬라이드 7: SPADI 아키텍처 개요

```
SPADI: 공간지능 플랫폼

SPADI (Spatial-Physical Agentic Device Interface)

"멀티모달 센서 융합을 통해 3D 공간을 인지하고,
 AI로 의미를 이해하며,
 Physical AI 시스템에 행동 명령을 전달하는
 완전한 공간지능 파이프라인"


3단계 아키텍처

1. Spatial Reconstruction (공간 재구성)
   └─ 멀티모달 센서 융합 → 고정밀 3D 재구성
   └─ Real2Sim: 실제 환경 → 시뮬레이션 변환

2. Spatial Understanding (공간 이해)
   └─ 객체 감지, 추적, 의미론적 매핑
   └─ "무엇이 어디에 있는가" 파악

3. Spatial Reasoning (공간 추론)
   └─ LLM 기반 자율 의사결정
   └─ "무엇을 할 것인가" 판단


핵심 구성
- 데이터 수집: LiDAR + RGB/열화상 카메라 + IMU
- 실시간 처리: NVIDIA Jetson Thor 플랫폼
- 지능형 분석: LLM + 컴퓨터 비전 + 딥러닝
```

**발표 멘트**:

> 이제 우리의 핵심 기술인 SPADI를 소개하겠습니다.
>
> SPADI는 Spatial-Physical Agentic Device Interface의 약자로, 멀티모달 센서 융합을 통해 3D 공간을 인지하고, AI로 의미를 이해하며, Physical AI 시스템에 행동 명령을 전달하는 완전한 공간지능 파이프라인입니다.
>
> SPADI는 3단계 아키텍처로 구성됩니다.
>
> 1단계, Spatial Reconstruction. 공간을 재구성합니다. 멀티모달 센서 융합을 통해 고정밀 3D를 만들고, 실제 환경을 시뮬레이션으로 변환합니다.
>
> 2단계, Spatial Understanding. 공간을 이해합니다. 객체를 감지하고 추적하며, "무엇이 어디에 있는가"를 파악합니다.
>
> 3단계, Spatial Reasoning. 공간에서 추론합니다. LLM 기반으로 자율적으로 의사결정하고, "무엇을 할 것인가"를 판단합니다.
>
> 이 모든 것이 NVIDIA Jetson Thor 플랫폼에서 실시간으로 처리됩니다.

---

### 슬라이드 8-9: Spatial Reconstruction - 공간을 재구성한다

```
Spatial Reconstruction: 공간을 재구성한다

핵심 기술

1. 멀티모달 센서 융합
   ├─ LiDAR: 정밀한 거리 및 3D 기하학 정보
   ├─ RGB 카메라: 색상, 질감, 세밀한 시각 정보
   ├─ 열화상 카메라: 온도 정보, 저조도 환경 대응
   └─ IMU: 센서 자세 및 움직임 정보

   → 센서 캘리브레이션 및 시점 동기화 (10ms 이내)

2. SLAM (Simultaneous Localization and Mapping)
   └─ 실시간 3D 지도 생성 및 정확한 위치 추정
   └─ 기하학적 정보 추출

3. 3D Gaussian Splatting
   └─ 실시간 사실적 렌더링
   └─ PSNR 27dB 이상 고품질 3D 모델

4. Real2Sim 파이프라인
   └─ NVIDIA Isaac Sim 및 Omniverse 통합
   └─ 실제 환경 → 시뮬레이션 자동 변환
   └─ 디지털 트윈 생성


실제 프로젝트 적용 사례

프로젝트: 4족 로봇 탑재형 멀티모달 센서 통합 시스템 (정부 과제)
기간: 2025-2026 (2차년도)

├─ 다관절 짐벌 기반 멀티모달 센서 모듈 개발
│  └─ LiDAR + RGB + 열화상 + IMU 통합
│  └─ 멀티모달 융합 데이터 획득 속도: 20Hz 이상
│  └─ 데이터 동기화 오차: 10ms 이내
│
├─ 3D Gaussian Splatting 기반 고정밀 맵핑
│  └─ 사실적 3D 모델 재구성
│  └─ 3D 렌더링 품질 (PSNR): 27dB 이상
│  └─ 실시간 상황 인지 및 시각화
│
└─ 무인 순찰/경계 실증
   └─ 중요시설 자율 순찰 및 장애물 회피
   └─ 침입 탐지 및 추적 (저조도 환경)
   └─ 환경 이상 감지 (온도 이상, 화재/연기)


대외 활동: GTC 2026 포스터 발표
- 주제: "Spatial Intelligence Architecture for Physical AI"
- NVIDIA 글로벌 AI 컨퍼런스에서 기술 소개
- 3단계 아키텍처 (Reconstruction → Understanding → Reasoning) 발표


기술 로드맵: 1단계 (2025년)
목표: 멀티모달 센서 모듈 완성
- NVIDIA Jetson Thor 플랫폼 통합
- 센서 융합 알고리즘 개발
- ROS2 기반 데이터 출력 파이프라인
```

**발표 멘트**:

> 첫 번째 단계, Spatial Reconstruction, 공간을 재구성합니다.
>
> 우리는 4가지 핵심 기술을 사용합니다.
>
> 먼저 멀티모달 센서 융합입니다. LiDAR로 정밀한 거리와 3D 기하학 정보를 얻고, RGB 카메라로 색상과 질감을 파악하며, 열화상 카메라로 온도 정보와 저조도 환경에서도 작동하게 하고, IMU로 센서의 자세를 파악합니다. 이 모든 센서를 정밀하게 캘리브레이션하고 10ms 이내로 동기화합니다.
>
> 두 번째는 SLAM입니다. 실시간으로 3D 지도를 생성하고 정확한 위치를 추정합니다.
>
> 세 번째는 3D Gaussian Splatting입니다. 실시간으로 사실적인 3D 렌더링을 수행하며, PSNR 27dB 이상의 고품질을 달성합니다.
>
> 네 번째는 Real2Sim 파이프라인입니다. NVIDIA Isaac Sim과 Omniverse를 통합하여 실제 환경을 시뮬레이션으로 자동 변환하고 디지털 트윈을 생성합니다.
>
> 이 기술들은 현재 진행 중인 정부 과제에 실제로 적용되고 있습니다. 4족 로봇에 탑재할 멀티모달 센서 통합 시스템을 개발하고 있으며, 20Hz 이상의 융합 데이터 획득 속도, 10ms 이내의 동기화 오차, 27dB 이상의 렌더링 품질을 목표로 하고 있습니다.
>
> 이 시스템은 중요시설 무인 순찰, 침입 탐지, 저조도 환경 추적, 화재나 연기 같은 환경 이상 감지에 사용됩니다.
>
> 또한 이 기술은 올해 NVIDIA GTC 2026 글로벌 AI 컨퍼런스에서 포스터로 발표될 예정입니다.

---

### 슬라이드 10-11: Spatial Understanding - 공간을 이해한다

```
Spatial Understanding: 공간을 이해한다

핵심 기술

1. 고정밀 객체 감지
   ├─ YOLOv8: 실시간 객체 감지
   └─ SAM2 (Segment Anything Model 2): 인스턴스 세그멘테이션

2. 사람 포즈 및 제스처 인식
   └─ MediaPipe: 33개 랜드마크 기반 스켈레톤 감지
   └─ 손동작, 표정, 자세 분석

3. 2D-to-3D 투영
   └─ LiDAR 포인트 클라우드 기반 3D 위치 매핑
   └─ 카메라 이미지의 2D 객체 → 3D 공간 좌표 변환

4. 다중 객체 추적
   ├─ Kalman Filter: 객체 위치 예측 및 보정
   ├─ Hungarian Algorithm: 최적 객체 매칭
   └─ 고유 ID 할당 및 지속적 추적

5. 의미론적 매핑 (Semantic Mapping)
   ├─ 객체 분류 (사람, 차량, 장애물 등)
   ├─ 공간 관계 분석 (거리, 방향, 상대 위치)
   └─ 이상 감지 (침입자, 쓰러진 사람, 분실물 등)

6. 하드웨어 가속
   └─ NVIDIA Isaac ROS 기반 실시간 처리
   └─ GPU 최적화를 통한 고성능 추론


실제 프로젝트 적용 사례

프로젝트: 3D Human Detection & Tracking System

기술 스택
- YOLOv8: 사람 감지 및 바운딩 박스 추출
- SAM2: 정밀한 인스턴스 세그멘테이션
- MediaPipe: 33개 랜드마크 기반 스켈레톤 추출
- Kalman Filter: 다중 사람 추적
- ROS2: 시스템 통합 및 데이터 파이프라인

센서 융합
- LiDAR: 3D 포인트 클라우드
- RGB 카메라: 2D 이미지

주요 기능
├─ 실시간 다중 사람 감지 및 추적
├─ 3D 공간 내 정확한 위치 파악
├─ 포즈 및 제스처 인식
├─ 사람별 고유 ID 할당 및 지속적 추적
└─ RViz2를 통한 3D 시각화

성능
- 실시간 처리 (30fps 이상)
- 고정밀 추적 (오차 10cm 이내)
- 다중 객체 동시 추적 (10명 이상)
- 제스처 인식 정확도 95% 이상


활용 사례
이 시스템은 다음 프로젝트들의 기반 기술로 활용:
- 4족 로봇 프로젝트: 침입자 탐지 및 추적
- 무인 리테일 시스템: 고객 위치 추적 및 행동 분석
- AI 서비스 에이전트: 사람 인식 및 접근 감지
```

**발표 멘트**:

> 두 번째 단계, Spatial Understanding, 공간을 이해합니다.
>
> 공간을 재구성했다면, 이제 그 안에 무엇이 있는지 이해해야 합니다.
>
> 우리는 6가지 핵심 기술을 사용합니다.
>
> 첫째, YOLOv8과 SAM2를 사용한 고정밀 객체 감지입니다. 실시간으로 객체를 감지하고 정밀하게 세그멘테이션합니다.
>
> 둘째, MediaPipe를 사용한 사람 포즈 및 제스처 인식입니다. 33개 랜드마크 기반으로 스켈레톤을 감지하고, 손동작, 표정, 자세를 분석합니다.
>
> 셋째, 2D-to-3D 투영입니다. 카메라로 본 2D 이미지의 객체를 LiDAR 포인트 클라우드를 사용해 3D 공간 좌표로 변환합니다.
>
> 넷째, Kalman Filter와 Hungarian Algorithm을 사용한 다중 객체 추적입니다. 객체마다 고유 ID를 할당하고 지속적으로 추적합니다.
>
> 다섯째, 의미론적 매핑입니다. 객체를 분류하고, 공간 관계를 분석하며, 이상 상황을 감지합니다.
>
> 여섯째, NVIDIA Isaac ROS 기반 하드웨어 가속으로 실시간 처리를 가능하게 합니다.
>
> 이 기술들을 통합한 대표적인 프로젝트가 바로 3D Human Detection & Tracking System입니다.
>
> 이 시스템은 LiDAR와 RGB 카메라를 융합하여, 실시간으로 여러 사람을 동시에 감지하고 추적합니다. 30fps 이상의 실시간 처리, 10cm 이내의 정확도, 10명 이상 동시 추적, 95% 이상의 제스처 인식 정확도를 달성했습니다.
>
> 이 시스템은 4족 로봇의 침입자 탐지, 무인 리테일의 고객 추적, AI 서비스 에이전트의 사람 인식 등 다양한 프로젝트의 기반 기술로 활용되고 있습니다.

---

### 슬라이드 12-13: Spatial Reasoning - 공간에서 판단하고 행동한다

```
Spatial Reasoning: 공간에서 판단하고 행동한다

핵심 기술

1. 자연어 명령 처리
   ├─ 멀티모달 LLM (GPT-4o 등)
   ├─ Transformer Engine을 통한 고속 추론
   └─ "침입자를 찾아서 추적해줘" 같은 복잡한 명령 이해

2. 시각-언어 융합 처리
   ├─ 카메라 영상 + 자연어 명령 통합
   ├─ Visual Question Answering
   └─ "저기 서 있는 사람이 뭘 하고 있어?" 같은 질의 응답

3. 멀티 에이전트 AI 아키텍처
   ├─ Triage Agent: 상황 분류 및 적절한 에이전트 선택
   ├─ Navigation Agent: 경로 계획 및 이동
   ├─ Execution Agent: 구체적 작업 수행
   └─ Conversation Agent: 대화 및 피드백

4. 컨텍스트 기반 자율 의사결정
   ├─ 로봇 상태 (배터리, 위치, 센서 상태)
   ├─ 환경 조건 (시간, 날씨, 조도)
   ├─ 작업 이력 (과거 수행한 작업, 학습 데이터)
   └─ 공간 제약 (장애물, 금지 구역, 안전 거리)
   → 종합적으로 판단하여 최적의 행동 결정

5. 행동 계획 수립 및 실행
   └─ Physical AI 시스템에 구체적 명령 생성
   └─ ROS2를 통한 로봇 제어

6. 동적 환경 대응
   └─ 실시간 환경 변화 감지 및 계획 재수립
   └─ "침입자가 도망갔어" → 즉시 추적 경로 변경


실제 프로젝트 적용 사례 1: AI Service Agent

프로젝트: 카페 무인 서비스 시스템

기술 스택
- OpenAI GPT-4.1: 자연어 이해 및 대화
- Streamlit: 웹 기반 UI
- ROS2: 로봇 시스템 통합
- OpenAI STT/TTS: 음성 입출력

멀티 에이전트 아키텍처
├─ Triage Agent: 고객 의도 파악 ("주문? 질문? 불만?")
├─ Order Agent: 주문 접수 및 장바구니 관리
├─ Service Agent: 서빙, 정리, 안내
└─ Conversation Agent: 일반 대화 및 추천

주요 기능
├─ 자연어 기반 주문 ("아메리카노 2잔이랑 카페라떼 1잔 주세요")
├─ 맥락 이해 ("아까 주문한 거 취소하고 싶어요")
├─ 능동적 추천 ("날씨가 더운데 시원한 음료 어때요?")
├─ 멤버십 연동 및 포인트 적립
└─ 실시간 스트리밍 AI 응답

특징
- 단순 명령 수행이 아닌 맥락 이해
- 언어적 + 비언어적 정보 종합 (표정, 제스처)
- 상황에 맞는 능동적 응대


실제 프로젝트 적용 사례 2: 무인 리테일 시스템

프로젝트: 공간지능 플랫폼 기반 무인 리테일 운영 시스템 (AI 챔피언 대회 제안)

핵심 개념
멀티모달 센서 기반 지능형 상호작용 디바이스
- LiDAR + RGB/열화상 카메라를 매장 입구/천장에 설치
- 고객과 직접 상호작용하며 메뉴 제안, 주문 접수, 위치 추적

주요 기능

1. 사람 인지 및 감성적 응대
   ├─ LLM 기반 자연스러운 대화
   ├─ 표정 분석을 통한 감정 파악
   └─ 개인화된 메뉴 추천

2. 3차원 공간 내 객체 추적
   ├─ 실시간 3D 맵 생성 및 업데이트
   ├─ 고객 및 사물 위치 정밀 추적
   └─ "2번 테이블 고객" 자동 식별

3. 주문 매칭 및 서빙 알림
   ├─ 좌석-주문 자동 매칭
   ├─ 서빙 로봇 연동
   └─ "2번 테이블, 아메리카노 나왔습니다" 알림

4. 매장 환경 관리
   ├─ 이상 상황 자동 감지 (분실물, 오염, 위험 상황)
   └─ 관리자 실시간 알림

차별점
- 멀티모달 센서 융합: 동적 환경, 조도 변화, 소음 속에서도 정확
- LLM 기반 감성형 AI: 언어적 + 비언어적 맥락 종합 이해
- 확장 가능한 설계: 고정형 → 이동형 로봇으로 확장

기대 효과
- 24시간 무인 운영, 인건비 절감
- 데이터 기반 고객 분석
- 디지털 격차 해소 (음성/제스처 인터페이스)
```

**발표 멘트**:

> 세 번째 단계, Spatial Reasoning, 공간에서 판단하고 행동합니다.
>
> 공간을 재구성하고 이해했다면, 이제 그 안에서 무엇을 할 것인지 자율적으로 판단해야 합니다.
>
> 우리는 6가지 핵심 기술을 사용합니다.
>
> 첫째, GPT-4 같은 멀티모달 LLM을 사용한 자연어 명령 처리입니다. "침입자를 찾아서 추적해줘" 같은 복잡한 명령을 이해합니다.
>
> 둘째, 시각-언어 융합 처리입니다. 카메라 영상과 자연어 명령을 통합하여 "저기 서 있는 사람이 뭘 하고 있어?" 같은 질문에 답합니다.
>
> 셋째, 멀티 에이전트 아키텍처입니다. Triage, Navigation, Execution, Conversation 등 여러 에이전트가 협력하여 작업을 수행합니다.
>
> 넷째, 컨텍스트 기반 자율 의사결정입니다. 로봇 상태, 환경 조건, 작업 이력, 공간 제약을 종합적으로 판단하여 최적의 행동을 결정합니다.
>
> 다섯째, 행동 계획을 수립하고 ROS2를 통해 Physical AI 시스템에 구체적 명령을 전달합니다.
>
> 여섯째, 동적 환경에 실시간으로 대응합니다. 침입자가 도망가면 즉시 추적 경로를 변경하는 식입니다.
>
> 이 기술들을 적용한 대표적인 프로젝트가 AI Service Agent입니다.
>
> 카페 무인 서비스 시스템으로, Triage, Order, Service, Conversation 에이전트가 협력하여 고객을 응대합니다. 단순히 명령을 수행하는 것이 아니라 맥락을 이해하고 능동적으로 응대합니다. 예를 들어 "날씨가 더운데 시원한 음료 어때요?" 같은 추천도 합니다.
>
> 또한 현재 기획 중인 무인 리테일 시스템도 이 기술을 활용합니다.
>
> 매장 입구와 천장에 설치된 멀티모달 센서가 고객과 직접 상호작용하며 메뉴를 제안하고, 주문을 접수하고, 위치를 추적합니다. 3D 공간에서 고객을 추적하여 "2번 테이블 고객"을 자동으로 식별하고, 서빙 로봇과 연동하여 "2번 테이블, 아메리카노 나왔습니다" 같은 알림을 보냅니다.
>
> 이 시스템의 차별점은 멀티모달 센서 융합으로 동적 환경, 조도 변화, 소음 속에서도 정확하게 작동하고, LLM 기반 감성형 AI로 언어적, 비언어적 맥락을 종합적으로 이해한다는 것입니다.

---

### 슬라이드 14: 왜 멀티모달 센서 융합인가?

```
왜 멀티모달 센서 융합인가?

단일 센서의 한계

카메라만 사용
├─ 조도 변화에 취약 (야간, 역광)
├─ 3D 정보 부족 (거리 측정 어려움)
└─ 투명/반사 물체 인식 한계

LiDAR만 사용
├─ 색상/질감 정보 부족
├─ 세밀한 객체 인식 한계 (얼굴 표정 불가)
└─ 비용이 높음

열화상만 사용
├─ 해상도 낮음
├─ 환경 맥락 파악 어려움
└─ 동일 온도 객체 구분 어려움


멀티모달 융합의 강점

LiDAR + RGB + 열화상 + IMU

1. 동적 환경 변화 대응
   └─ 조도, 날씨, 시간대 변화에도 안정적 인지
   └─ 낮에는 RGB, 밤에는 열화상 보완

2. 종합적 공간 이해
   └─ 언어적 명령 + 비언어적 맥락 (표정, 움직임) 통합
   └─ "무엇을" (RGB) + "어디에" (LiDAR) + "어떤 상태" (열화상)

3. 정밀한 객체 인식
   └─ 3D 위치 (LiDAR) + 색상/질감 (RGB) + 온도 (열화상)
   └─ "사람"을 넘어 "쓰러진 사람", "침입자" 구분 가능

4. 강건한 추적
   └─ 센서 간 상호 보완
   └─ 하나의 센서가 실패해도 다른 센서로 보완


실증 사례: 4족 로봇 프로젝트

환경 조건                단일 센서       멀티모달 융합
─────────────────────────────────────────────────
저조도 야간 순찰         카메라 ✗        열화상 + LiDAR ✓
비/안개 환경             카메라 ✗        LiDAR + 열화상 ✓
역광 상황                카메라 ✗        LiDAR + 열화상 ✓
화재/연기 감지           LiDAR ✗         열화상 ✓
정밀 거리 측정           카메라 ✗        LiDAR ✓
얼굴/표정 인식           LiDAR ✗         RGB ✓


결론
멀티모달 센서 융합은 단순한 기술적 선택이 아닌,
Physical AI가 다양한 환경에서 안정적으로 작동하기 위한 필수 요소
```

**발표 멘트**:

> 여기서 질문이 생길 수 있습니다. "왜 굳이 여러 센서를 융합해야 하나요? 카메라 하나만 쓰면 안 되나요?"
>
> 답은 간단합니다. 단일 센서로는 한계가 있기 때문입니다.
>
> 카메라만 사용하면 조도 변화에 취약하고, 3D 정보가 부족하며, 투명하거나 반사되는 물체를 인식하기 어렵습니다.
>
> LiDAR만 사용하면 색상과 질감 정보가 없고, 얼굴 표정 같은 세밀한 인식이 불가능합니다.
>
> 열화상만 사용하면 해상도가 낮고, 환경 맥락을 파악하기 어렵습니다.
>
> 하지만 LiDAR, RGB, 열화상, IMU를 융합하면 이야기가 달라집니다.
>
> 첫째, 동적 환경 변화에 대응할 수 있습니다. 낮에는 RGB를 주로 쓰고, 밤에는 열화상으로 보완합니다.
>
> 둘째, 종합적으로 공간을 이해합니다. "무엇을" RGB로, "어디에" LiDAR로, "어떤 상태" 열화상으로 파악합니다.
>
> 셋째, 정밀한 객체 인식이 가능합니다. 단순히 "사람"을 넘어 "쓰러진 사람", "침입자"를 구분할 수 있습니다.
>
> 넷째, 강건한 추적이 가능합니다. 하나의 센서가 실패해도 다른 센서로 보완합니다.
>
> 실제로 4족 로봇 프로젝트에서 저조도 야간 순찰, 비나 안개 환경, 역광 상황, 화재나 연기 감지 등 다양한 환경에서 안정적으로 작동하는 것을 검증하고 있습니다.
>
> 결론적으로, 멀티모달 센서 융합은 단순한 기술적 선택이 아닌, Physical AI가 다양한 환경에서 안정적으로 작동하기 위한 필수 요소입니다.

---

## 5. 기술 로드맵과 미래 (3-4분, 2-3장)

### 슬라이드 15: 기술 로드맵

```
SPADI 기술 로드맵

1단계 (2025년): 멀티모달 센서 모듈
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
목표
└─ 멀티모달 센서 융합 데이터 출력

핵심 구성
├─ LiDAR + RGB/열화상 카메라 + IMU 통합 센서
├─ NVIDIA Jetson Thor 플랫폼 통합
├─ 센서 융합 알고리즘 개발
└─ ROS2 기반 데이터 출력 파이프라인

성과물
└─ 멀티모달 융합 센서 하드웨어 모듈
└─ 20Hz 이상 데이터 획득
└─ 10ms 이내 동기화


2단계 (2026년): Plug-and-Play 자율주행 모듈
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
목표
└─ 다양한 로봇 플랫폼에 즉시 장착 가능한 자율주행 솔루션

핵심 기능
├─ 4족, 2족, 바퀴형 로봇 범용 호환 시스템
├─ LLM 기반 자연어 명령 처리
├─ Isaac ROS 자율주행 스택 통합
└─ 클라우드 연동 및 OTA 업데이트

타겟 시장
└─ 보안/순찰: 무인 경비, 침입 탐지
└─ 리테일: 무인 매장, 고객 서비스
└─ 산업: 작업 로봇, 안전 관리


3단계 (2027-2028년): 완전한 공간지능 플랫폼
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
목표
└─ Spatial AI 기반 고도화된 공간 이해 및 의사결정

핵심 기능
├─ Real2Sim 기술 완성
│  └─ 실제 환경 → 시뮬레이션 자동 변환
│  └─ 디지털 트윈 기반 AI 학습
│
├─ Gaussian Splatting 실시간 최적화
│  └─ 사실적 3D 렌더링
│  └─ 고정밀 공간 재구성
│
├─ Semantic Mapping 고도화
│  └─ 공간의 의미적 이해
│  └─ 컨텍스트 기반 추론
│
├─ AI 에이전트 시스템
│  └─ 자율적 의사결정
│  └─ 멀티 에이전트 협업
│
└─ NVIDIA Omniverse 완전 통합
   └─ 글로벌 플랫폼 호환성
   └─ 산업 표준 준수

확장 분야
└─ 스마트 시티, 스마트홈, 자율주행 인프라
```

**발표 멘트**:

> 이제 우리의 기술 로드맵을 말씀드리겠습니다.
>
> 1단계는 2025년, 올해입니다. 멀티모달 센서 모듈을 완성하는 단계입니다. LiDAR, RGB, 열화상, IMU를 통합한 센서 하드웨어 모듈을 만들고, 20Hz 이상의 데이터 획득, 10ms 이내 동기화를 달성합니다.
>
> 2단계는 2026년입니다. Plug-and-Play 자율주행 모듈을 개발합니다. 4족, 2족, 바퀴형 로봇 등 다양한 플랫폼에 즉시 장착 가능하고, LLM 기반 자연어 명령을 처리하며, 클라우드 연동과 OTA 업데이트를 지원합니다. 보안, 리테일, 산업 분야를 타겟으로 합니다.
>
> 3단계는 2027~2028년입니다. 완전한 공간지능 플랫폼을 구축합니다. Real2Sim 기술 완성, Gaussian Splatting 실시간 최적화, Semantic Mapping 고도화, AI 에이전트 시스템, NVIDIA Omniverse 완전 통합을 목표로 합니다. 그리고 스마트 시티, 스마트홈, 자율주행 인프라 등으로 확장합니다.

---

### 슬라이드 16: 단기/중기/장기 목표

```
IVC의 미래 계획

단기 목표 (2025-2026년)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. 멀티모달 센서 플랫폼 완성
   ├─ 센서 융합 알고리즘 고도화
   ├─ Real2Sim 파이프라인 구축
   └─ Gaussian Splatting 실시간 최적화

2. 정부 과제 성공적 완수
   ├─ 4족 로봇 프로젝트 실증 검증
   └─ 기술성숙도 TRL 8단계 달성

3. 무인 리테일 시스템 구현
   ├─ AI 챔피언 대회 참가
   └─ 프로토타입 제작 및 시연


중기 목표 (2027-2028년)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

1. 공간지능 플랫폼 상용화
   ├─ 범용 호환 인터페이스 제공
   ├─ 다양한 로봇 플랫폼 연동 (4족, 2족, AMR 등)
   └─ Plug-and-Play 방식 지원

2. 확장 분야 개척
   ├─ 리테일: 무인 매장, 스마트 스토어
   ├─ 헬스케어: 고령자 케어, 병원 안내
   ├─ 보안: 무인 순찰, 침입 탐지
   └─ 산업: 작업 로봇, 안전 관리

3. 특허 및 논문 성과
   ├─ 핵심 기술 특허 출원
   └─ AI/로보틱스 분야 국제 학술지 게재


장기 비전
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

"멀티모달 센서 기반 공간지능 플랫폼의 글로벌 리더"

├─ Physical AI 생태계의 핵심 인터페이스 제공자
├─ 다양한 산업군으로 기술 확산
├─ 스마트 시티, 스마트홈, 자율주행 등 차세대 인프라 기반 기술 확립
└─ 국내 기술 수준을 세계 최고 수준으로 도약


핵심 가치 제안

Physical AI 제조사들에게
"우리 로봇에 SPADI를 장착하면,
 어떤 환경에서도 안정적으로 공간을 이해하고
 자율적으로 판단하고 행동할 수 있습니다"

산업 고객들에게
"센서 선택, 융합, AI 통합의 복잡함 없이
 Plug-and-Play 방식으로 즉시 공간지능을 활용하세요"
```

**발표 멘트**:

> IVC의 미래 계획입니다.
>
> 단기 목표로는 멀티모달 센서 플랫폼을 완성하고, 정부 과제를 성공적으로 완수하며, 무인 리테일 시스템을 구현합니다.
>
> 중기 목표로는 공간지능 플랫폼을 상용화하고, 리테일, 헬스케어, 보안, 산업 등 다양한 분야로 확장하며, 특허와 논문 성과를 냅니다.
>
> 장기 비전은 "멀티모달 센서 기반 공간지능 플랫폼의 글로벌 리더"가 되는 것입니다. Physical AI 생태계의 핵심 인터페이스 제공자가 되고, 스마트 시티, 스마트홈, 자율주행 등 차세대 인프라의 기반 기술을 확립하며, 국내 기술 수준을 세계 최고 수준으로 끌어올리겠습니다.
>
> 우리의 핵심 가치 제안은 이것입니다.
>
> Physical AI 제조사들에게는 "우리 로봇에 SPADI를 장착하면, 어떤 환경에서도 안정적으로 공간을 이해하고 자율적으로 판단하고 행동할 수 있습니다"
>
> 산업 고객들에게는 "센서 선택, 융합, AI 통합의 복잡함 없이, Plug-and-Play 방식으로 즉시 공간지능을 활용하세요"

---

## 6. Closing (1분, 1장)

### 슬라이드 17: 마무리

```
IVC (혁신비전센터)

"Spatial AI로 Physical AI를 완성한다"


우리의 미션
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

센서가 공간을 스캔하는 것을 넘어,
공간을 이해하게 만든다

Physical AI가 "움직이는 것"을 넘어,
"보고, 생각하고, 판단"하게 만든다


우리의 접근
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

Spatial Reconstruction → Understanding → Reasoning

멀티모달 센서 융합 + AI 기반 공간지능 플랫폼

Hardware + Software 완전 통합


IVC는 멀티모달 센서 융합과 AI 기술을 통해
공간을 이해하고, 판단하고, 행동하는
차세대 공간지능 플랫폼을 만들어갑니다.


Physical AI 시대, 우리가 만들어갑니다.



Q & A
```

**발표 멘트**:

> 정리하겠습니다.
>
> IVC의 미션은 "Spatial AI로 Physical AI를 완성한다"입니다.
>
> 센서가 공간을 스캔하는 것을 넘어, 공간을 이해하게 만듭니다.
> Physical AI가 단순히 움직이는 것을 넘어, 보고, 생각하고, 판단하게 만듭니다.
>
> 우리는 Spatial Reconstruction, Understanding, Reasoning의 3단계로 이를 구현하며, 멀티모달 센서 융합과 AI 기반 공간지능 플랫폼, 그리고 하드웨어와 소프트웨어의 완전한 통합을 통해 이를 달성합니다.
>
> IVC는 멀티모달 센서 융합과 AI 기술을 통해 공간을 이해하고, 판단하고, 행동하는 차세대 공간지능 플랫폼을 만들어가고 있습니다.
>
> Physical AI 시대, 우리가 만들어갑니다.
>
> 경청해주셔서 감사합니다. 질문 받겠습니다.

---

## 📊 발표 시간 배분 요약

| 섹션                       | 슬라이드    | 시간        |
| -------------------------- | ----------- | ----------- |
| 1. Opening                 | 1장         | 1분         |
| 2. Physical AI 시대의 도래 | 3장         | 5-6분       |
| 3. IVC의 탄생과 비전       | 2장         | 3-4분       |
| 4. SPADI 공간지능 플랫폼   | 6장         | 12-13분     |
| 5. 기술 로드맵과 미래      | 2장         | 3-4분       |
| 6. Closing                 | 1장         | 1분         |
| **총계**                   | **15-17장** | **25-30분** |

---

## 💡 발표 Tips

### 발표 준비

1. 각 슬라이드마다 핵심 메시지 1-2개만 전달
2. 전문 용어는 쉽게 풀어서 설명 (신입 직원 대상)
3. 실제 프로젝트 사례를 통해 기술을 구체화
4. 데모 영상이 있다면 적극 활용 (3D 추적, AI 에이전트 등)

### 시간 조절

- 질문이 많을 것으로 예상되면 본론을 22-23분으로 축소
- 각 섹션 후 1-2개 질문 받는 것도 좋은 방법

### 청중 참여

- "여러분은 Physical AI 하면 뭐가 떠오르나요?" 같은 질문으로 시작
- 프로젝트 시연 영상을 보여주며 설명
- 마지막에 "IVC에서 일하고 싶다", "더 알고 싶다" 등의 반응 유도

### 시각 자료

- 센서 융합 다이어그램
- SPADI 3단계 아키텍처 그림
- 프로젝트 스크린샷/영상
- 로드맵 타임라인
- Before/After 비교 (단일 센서 vs 멀티모달)
